{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spam-filter-ml\n",
    "\n",
    "This series of notebooks are used to develop a full NLP pipeline (tokenization, lemmatization, vectorization, cross-validation/testing) for creating a spam filter. The model will be trained/tested using a dataset available on Kaggle: https://www.kaggle.com/uciml/sms-spam-collection-dataset.\n",
    "\n",
    "This notebook goes through the machine learning parts in the pipeline; for exploratory data analysis and data cleaning, see the *spam-filter* notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support as scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "\n",
    "The next step is to vectorize each text message so that the data can be fed to a machine learning model for training. I will be using the TF-IDF vectorizer (term frequency - inverse document frequency) from scikit-learn. This takes two terms into account to determine the weighting of each token: 1) the frequency of each token in each message, and 2) the frequency of each token in the overall dataset. The vectorizer requires an analyser, which I will pass as a cleaning function which combines all the previous steps. The dataset will then be split into a training and testing set. The vectorizer is fit using the training set, which will then be used to transform the overall dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text_len</th>\n",
       "      <th>text_punct</th>\n",
       "      <th>text_cap</th>\n",
       "      <th>raw_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>92</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.033</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>24</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.083</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>128</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.078</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>39</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.051</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>49</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  text_len  text_punct  text_cap  \\\n",
       "0   ham        92       0.098     0.033   \n",
       "1   ham        24       0.250     0.083   \n",
       "2  spam       128       0.047     0.078   \n",
       "3   ham        39       0.154     0.051   \n",
       "4   ham        49       0.041     0.041   \n",
       "\n",
       "                                                                                              raw_text  \n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...  \n",
       "1                                                                     Ok lar... Joking wif u oni...     \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...  \n",
       "3                                                 U dun say so early hor... U c already then say...     \n",
       "4                                     Nah I don't think he goes to usf, he lives around here though     "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('spam_updated.csv')\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [go, jurong, point, crazy, available, bugis, n, great, world, la, e, buffet, cine, got, amore, wat]\n",
       "1                                                                         [ok, lar, joking, wif, u, oni]\n",
       "2    [free, entry, 2, wkly, comp, win, fa, cup, final, tkts, 21st, may, 2005, text, fa, 87121, receiv...\n",
       "3                                                          [u, dun, say, early, hor, u, c, already, say]\n",
       "4                                                      [nah, dont, think, go, usf, life, around, though]\n",
       "Name: raw_text, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define clean_text function to be used as the analyser in the tfidf vectorizer\n",
    "def clean_text(text):\n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    text = ''.join([char.lower() for char in text if char not in string.punctuation])\n",
    "    tokenized_list = re.split('\\W+', text)\n",
    "    text = [wnl.lemmatize(word) for word in tokenized_list if word not in stopwords and word != '']\n",
    "    return text\n",
    "\n",
    "# Checking that clean_text function works\n",
    "df_test_clean = df['raw_text'].apply(lambda x: clean_text(x))\n",
    "df_test_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_text</th>\n",
       "      <th>text_len</th>\n",
       "      <th>text_punct</th>\n",
       "      <th>text_cap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2239</th>\n",
       "      <td>Every day i use to sleep after  &amp;lt;#&amp;gt;  so only.</td>\n",
       "      <td>40</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>Give me a sec to think think about it</td>\n",
       "      <td>29</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5136</th>\n",
       "      <td>There are some nice pubs near here or there is Frankie n Bennys near the warner cinema?</td>\n",
       "      <td>71</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4714</th>\n",
       "      <td>S:)8 min to go for lunch:)</td>\n",
       "      <td>21</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5491</th>\n",
       "      <td>U studying in sch or going home? Anyway i'll b going 2 sch later.</td>\n",
       "      <td>52</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                        raw_text  \\\n",
       "2239                                      Every day i use to sleep after  &lt;#&gt;  so only.      \n",
       "1005                                                    Give me a sec to think think about it      \n",
       "5136  There are some nice pubs near here or there is Frankie n Bennys near the warner cinema?      \n",
       "4714                                                               S:)8 min to go for lunch:)      \n",
       "5491                        U studying in sch or going home? Anyway i'll b going 2 sch later.      \n",
       "\n",
       "      text_len  text_punct  text_cap  \n",
       "2239        40       0.150     0.025  \n",
       "1005        29       0.000     0.034  \n",
       "5136        71       0.014     0.042  \n",
       "4714        21       0.190     0.048  \n",
       "5491        52       0.058     0.038  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data into features (X) and labels (y), then split into training/testing sets\n",
    "X = df[['raw_text', 'text_len', 'text_punct', 'text_cap']]\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer=<function clean_text at 0x000001135597D8C8>,\n",
       "        binary=False, decode_error='strict', dtype=<class 'numpy.int64'>,\n",
       "        encoding='utf-8', input='content', lowercase=True, max_df=1.0,\n",
       "        max_features=None, min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "        preprocessor=None, smooth_idf=True, stop_words=None,\n",
       "        strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialise the vectorizer, fit using raw text in training data\n",
    "tfidf = TfidfVectorizer(analyzer=clean_text)\n",
    "tfidf_fit = tfidf.fit(X_train['raw_text'])\n",
    "tfidf_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089my</th>\n",
       "      <th>0121</th>\n",
       "      <th>01223585236</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>02</th>\n",
       "      <th>020603</th>\n",
       "      <th>0207</th>\n",
       "      <th>02070836089</th>\n",
       "      <th>...</th>\n",
       "      <th>ìïll</th>\n",
       "      <th>ó</th>\n",
       "      <th>ö</th>\n",
       "      <th>û</th>\n",
       "      <th>ûthanks</th>\n",
       "      <th>ûªm</th>\n",
       "      <th>ûªt</th>\n",
       "      <th>ûï</th>\n",
       "      <th>ûò</th>\n",
       "      <th>ûówell</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7953 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  008704050406  0089my  0121  01223585236  01223585334   02  020603  \\\n",
       "0  0.0           0.0     0.0   0.0          0.0          0.0  0.0     0.0   \n",
       "1  0.0           0.0     0.0   0.0          0.0          0.0  0.0     0.0   \n",
       "2  0.0           0.0     0.0   0.0          0.0          0.0  0.0     0.0   \n",
       "3  0.0           0.0     0.0   0.0          0.0          0.0  0.0     0.0   \n",
       "4  0.0           0.0     0.0   0.0          0.0          0.0  0.0     0.0   \n",
       "\n",
       "   0207  02070836089   ...    ìïll    ó    ö    û  ûthanks  ûªm  ûªt   ûï  \\\n",
       "0   0.0          0.0   ...     0.0  0.0  0.0  0.0      0.0  0.0  0.0  0.0   \n",
       "1   0.0          0.0   ...     0.0  0.0  0.0  0.0      0.0  0.0  0.0  0.0   \n",
       "2   0.0          0.0   ...     0.0  0.0  0.0  0.0      0.0  0.0  0.0  0.0   \n",
       "3   0.0          0.0   ...     0.0  0.0  0.0  0.0      0.0  0.0  0.0  0.0   \n",
       "4   0.0          0.0   ...     0.0  0.0  0.0  0.0      0.0  0.0  0.0  0.0   \n",
       "\n",
       "    ûò  ûówell  \n",
       "0  0.0     0.0  \n",
       "1  0.0     0.0  \n",
       "2  0.0     0.0  \n",
       "3  0.0     0.0  \n",
       "4  0.0     0.0  \n",
       "\n",
       "[5 rows x 7953 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use vectorizer to transform both training and testing set; returns a sparse matrix, need to use .toarray() method to read\n",
    "tfidf_train = pd.DataFrame(tfidf_fit.transform(X_train['raw_text']).toarray())\n",
    "tfidf_test = pd.DataFrame(tfidf_fit.transform(X_test['raw_text']).toarray())\n",
    "\n",
    "# Rename vectorized columns with names of tokens\n",
    "tfidf_train.columns = tfidf_fit.get_feature_names()\n",
    "tfidf_test.columns = tfidf_fit.get_feature_names()\n",
    "tfidf_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_len</th>\n",
       "      <th>text_punct</th>\n",
       "      <th>text_cap</th>\n",
       "      <th>0</th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089my</th>\n",
       "      <th>0121</th>\n",
       "      <th>01223585236</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>02</th>\n",
       "      <th>...</th>\n",
       "      <th>ìïll</th>\n",
       "      <th>ó</th>\n",
       "      <th>ö</th>\n",
       "      <th>û</th>\n",
       "      <th>ûthanks</th>\n",
       "      <th>ûªm</th>\n",
       "      <th>ûªt</th>\n",
       "      <th>ûï</th>\n",
       "      <th>ûò</th>\n",
       "      <th>ûówell</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>71</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7956 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   text_len  text_punct  text_cap    0  008704050406  0089my  0121  \\\n",
       "0        40       0.150     0.025  0.0           0.0     0.0   0.0   \n",
       "1        29       0.000     0.034  0.0           0.0     0.0   0.0   \n",
       "2        71       0.014     0.042  0.0           0.0     0.0   0.0   \n",
       "3        21       0.190     0.048  0.0           0.0     0.0   0.0   \n",
       "4        52       0.058     0.038  0.0           0.0     0.0   0.0   \n",
       "\n",
       "   01223585236  01223585334   02   ...    ìïll    ó    ö    û  ûthanks  ûªm  \\\n",
       "0          0.0          0.0  0.0   ...     0.0  0.0  0.0  0.0      0.0  0.0   \n",
       "1          0.0          0.0  0.0   ...     0.0  0.0  0.0  0.0      0.0  0.0   \n",
       "2          0.0          0.0  0.0   ...     0.0  0.0  0.0  0.0      0.0  0.0   \n",
       "3          0.0          0.0  0.0   ...     0.0  0.0  0.0  0.0      0.0  0.0   \n",
       "4          0.0          0.0  0.0   ...     0.0  0.0  0.0  0.0      0.0  0.0   \n",
       "\n",
       "   ûªt   ûï   ûò  ûówell  \n",
       "0  0.0  0.0  0.0     0.0  \n",
       "1  0.0  0.0  0.0     0.0  \n",
       "2  0.0  0.0  0.0     0.0  \n",
       "3  0.0  0.0  0.0     0.0  \n",
       "4  0.0  0.0  0.0     0.0  \n",
       "\n",
       "[5 rows x 7956 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate vectorized text and other features into training and testing dataframes\n",
    "X_train_vect = pd.concat([X_train[['text_len', 'text_punct', 'text_cap']].reset_index(drop=True), tfidf_train], axis=1)\n",
    "X_test_vect = pd.concat([X_test[['text_len', 'text_punct', 'text_cap']].reset_index(drop=True), tfidf_test], axis=1)\n",
    "X_train_vect.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "\n",
    "The dataset is now ready to be used to train and test various machine learning models. For this part of the project I will be using both the Random Forest and Gradient Boosted Tree algorithms. A grid search is performed to tune the hyperparameters of each model to find the one that has the best performance. Common metrics for classification tasks include precision, recall, accuracy, and the F1 score. However, given that this ML model is used for a spam filter, it is ideal to minimise the false positives (i.e. it is more important that 'real' texts/emails are not incorrectly classified as spam; spam that is misclassified as ham presents a lesser impact in a real world setting). Therefore, precision is arguably the most important performance metric for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 0.481s, Pred time: 0.184s | Precision: 0.992, Recall: 0.812, F1: 0.893\n"
     ]
    }
   ],
   "source": [
    "# Train using X_train_vect, y_train; make predictions on X_test_vect, y_test\n",
    "\n",
    "RFC = RandomForestClassifier(n_jobs=-1) # Run jobs in parallel; use default hyperparameters for now\n",
    "\n",
    "fit_start = time.time()\n",
    "RFC_model = RFC.fit(X_train_vect, y_train) \n",
    "fit_end = time.time()\n",
    "fit_time = fit_end - fit_start\n",
    "\n",
    "pred_start = time.time()\n",
    "RFC_y_pred = RFC_model.predict(X_test_vect)\n",
    "pred_end = time.time()\n",
    "pred_time = pred_end - pred_start\n",
    "\n",
    "# Calculate precision/recall/f1 scores by comparing predictions and test data; use 'spam' as the positive label\n",
    "precision, recall, f1, support = scores(y_test, RFC_y_pred, pos_label='spam', average='binary')\n",
    "print('Fit time: {}s, Pred time: {}s | Precision: {}, Recall: {}, F1: {}'.format(\n",
    "    round(fit_time, 3), round(pred_time, 3), round(precision, 3), round(recall, 3), round(f1, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 35.718s, Pred time: 0.107s | Precision: 0.992, Recall: 0.819, F1: 0.897\n"
     ]
    }
   ],
   "source": [
    "# Repeat process using gradient boosted trees\n",
    "\n",
    "GBT = GradientBoostingClassifier() # Run jobs in parallel; use default hyperparameters for now\n",
    "\n",
    "fit_start = time.time()\n",
    "GBT_model = GBT.fit(X_train_vect, y_train) \n",
    "fit_end = time.time()\n",
    "fit_time = fit_end - fit_start\n",
    "\n",
    "pred_start = time.time()\n",
    "GBT_y_pred = GBT_model.predict(X_test_vect)\n",
    "pred_end = time.time()\n",
    "pred_time = pred_end - pred_start\n",
    "\n",
    "# Calculate precision/recall/f1 scores by comparing predictions and test data; use 'spam' as the positive label\n",
    "precision, recall, f1, support = scores(y_test, GBT_y_pred, pos_label='spam', average='binary')\n",
    "print('Fit time: {}s, Pred time: {}s | Precision: {}, Recall: {}, F1: {}'.format(\n",
    "    round(fit_time, 3), round(pred_time, 3), round(precision, 3), round(recall, 3), round(f1, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 461.301s, Pred time: 22.981s | Precision: 0.993, Recall: 0.881, F1: 0.934\n"
     ]
    }
   ],
   "source": [
    "# Repeat process using support vector machine\n",
    "\n",
    "SVM = SVC(kernel='linear') # Run jobs in parallel; use default hyperparameters for now\n",
    "\n",
    "fit_start = time.time()\n",
    "SVM_model = SVM.fit(X_train_vect, y_train) \n",
    "fit_end = time.time()\n",
    "fit_time = fit_end - fit_start\n",
    "\n",
    "pred_start = time.time()\n",
    "SVM_y_pred = SVM_model.predict(X_test_vect)\n",
    "pred_end = time.time()\n",
    "pred_time = pred_end - pred_start\n",
    "\n",
    "# Calculate precision/recall/f1 scores by comparing predictions and test data; use 'spam' as the positive label\n",
    "precision, recall, f1, support = scores(y_test, SVM_y_pred, pos_label='spam', average='binary')\n",
    "print('Fit time: {}s, Pred time: {}s | Precision: {}, Recall: {}, F1: {}'.format(\n",
    "    round(fit_time, 3), round(pred_time, 3), round(precision, 3), round(recall, 3), round(f1, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 67.321s, Pred time: 0.094s | Precision: 0.993, Recall: 0.906, F1: 0.948\n"
     ]
    }
   ],
   "source": [
    "# Repeat process using multilayer perceptron\n",
    "\n",
    "MLP = MLPClassifier() # Run jobs in parallel; use default hyperparameters for now\n",
    "\n",
    "fit_start = time.time()\n",
    "MLP_model = MLP.fit(X_train_vect, y_train) \n",
    "fit_end = time.time()\n",
    "fit_time = fit_end - fit_start\n",
    "\n",
    "pred_start = time.time()\n",
    "MLP_y_pred = MLP_model.predict(X_test_vect)\n",
    "pred_end = time.time()\n",
    "pred_time = pred_end - pred_start\n",
    "\n",
    "# Calculate precision/recall/f1 scores by comparing predictions and test data; use 'spam' as the positive label\n",
    "precision, recall, f1, support = scores(y_test, MLP_y_pred, pos_label='spam', average='binary')\n",
    "print('Fit time: {}s, Pred time: {}s | Precision: {}, Recall: {}, F1: {}'.format(\n",
    "    round(fit_time, 3), round(pred_time, 3), round(precision, 3), round(recall, 3), round(f1, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 0.581s, Pred time: 0.062s | Precision: 0.992, Recall: 0.775, F1: 0.87\n"
     ]
    }
   ],
   "source": [
    "# Repeat process using logistic regression\n",
    "\n",
    "LogReg = LogisticRegression() # Run jobs in parallel; use default hyperparameters for now\n",
    "\n",
    "fit_start = time.time()\n",
    "LogReg_model = LogReg.fit(X_train_vect, y_train) \n",
    "fit_end = time.time()\n",
    "fit_time = fit_end - fit_start\n",
    "\n",
    "pred_start = time.time()\n",
    "LogReg_y_pred = LogReg_model.predict(X_test_vect)\n",
    "pred_end = time.time()\n",
    "pred_time = pred_end - pred_start\n",
    "\n",
    "# Calculate precision/recall/f1 scores by comparing predictions and test data; use 'spam' as the positive label\n",
    "precision, recall, f1, support = scores(y_test, LogReg_y_pred, pos_label='spam', average='binary')\n",
    "print('Fit time: {}s, Pred time: {}s | Precision: {}, Recall: {}, F1: {}'.format(\n",
    "    round(fit_time, 3), round(pred_time, 3), round(precision, 3), round(recall, 3), round(f1, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that all models have a high precision score of 99.2% to 99.3%, meaning that their false positive rates (i.e. ham misclassified as spam) is relatively low. Their recall scores are generally lower, ranging from 77.5% to 90.6%, meaning that there is still some spam that the models are unable to identify correctly. The results heavily depend on how the dataset was split using train_test_split, so different runs may return difference scores (this issue can be tackled by performing cross-validation, which is incorporated into the GridSearchCV method.\n",
    "\n",
    "There is a general tradeoff between training time and accuracy. For example, the logistic regression model is one of the fastest models to train, but its recall score seems to be poor. Alternatively, the support vector classifier has precision and recall scores that are above average, but it takes a very long time to train. The multilayer perceptron takes an average amount of time to train, but gives better scores overall. Also note that the gradient boosting classifier takes much longer to fit compared to the random forest classifier, as the decision trees in GBT are constructed sequentially and are not independent from each other. Given these reasons, I will choose to further investigate the RFC, GBT, and MLP models.\n",
    "\n",
    "The next step is to perform a grid search on both of these algorithms to find the hyperparameters which optimise the overall accuracy of the spam filtering system, which will be done in a separate notebook *spam-filter-gridsearch*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
