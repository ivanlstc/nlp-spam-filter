{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spam-filter-ml\n",
    "\n",
    "This series of notebooks are used to develop a full NLP pipeline (tokenization, lemmatization, vectorization, cross-validation/testing) for creating a spam filter. The model will be trained/tested using a dataset available on Kaggle: https://www.kaggle.com/uciml/sms-spam-collection-dataset.\n",
    "\n",
    "This notebook goes through the machine learning parts in the pipeline; for exploratory data analysis and data cleaning, see the *spam-filter* notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "\n",
    "The next step is to vectorize each text message so that the data can be fed to a machine learning model for training. I will be using the TF-IDF vectorizer (term frequency - inverse document frequency) from scikit-learn that takes into account the frequency of each token in each message and in the overall dataset to determine their weighting. The vectorizer requires an analyser, which I will pass as a cleaning function which combines all the previous steps. The dataset will then be split into a training and testing set. The vectorizer is fit using the training set, which will then be used to transform the overall dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text_len</th>\n",
       "      <th>text_punct</th>\n",
       "      <th>text_cap</th>\n",
       "      <th>text_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>92</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.033</td>\n",
       "      <td>['go', 'jurong', 'point', 'crazy', 'available'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>24</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.083</td>\n",
       "      <td>['ok', 'lar', 'joking', 'wif', 'u', 'oni', '']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>128</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.078</td>\n",
       "      <td>['free', 'entry', '2', 'wkly', 'comp', 'win', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>39</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.051</td>\n",
       "      <td>['u', 'dun', 'say', 'early', 'hor', 'u', 'c', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>49</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>['nah', 'dont', 'think', 'go', 'usf', 'life', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  text_len  text_punct  text_cap  \\\n",
       "0   ham        92       0.098     0.033   \n",
       "1   ham        24       0.250     0.083   \n",
       "2  spam       128       0.047     0.078   \n",
       "3   ham        39       0.154     0.051   \n",
       "4   ham        49       0.041     0.041   \n",
       "\n",
       "                                        text_cleaned  \n",
       "0  ['go', 'jurong', 'point', 'crazy', 'available'...  \n",
       "1     ['ok', 'lar', 'joking', 'wif', 'u', 'oni', '']  \n",
       "2  ['free', 'entry', '2', 'wkly', 'comp', 'win', ...  \n",
       "3  ['u', 'dun', 'say', 'early', 'hor', 'u', 'c', ...  \n",
       "4  ['nah', 'dont', 'think', 'go', 'usf', 'life', ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('spam_cleaned.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['text_cleaned', 'text_len', 'text_punct', 'text_cap']]\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        ham\n",
       "1        ham\n",
       "2       spam\n",
       "3        ham\n",
       "4        ham\n",
       "5       spam\n",
       "6        ham\n",
       "7        ham\n",
       "8       spam\n",
       "9       spam\n",
       "10       ham\n",
       "11      spam\n",
       "12      spam\n",
       "13       ham\n",
       "14       ham\n",
       "15      spam\n",
       "16       ham\n",
       "17       ham\n",
       "18       ham\n",
       "19      spam\n",
       "20       ham\n",
       "21       ham\n",
       "22       ham\n",
       "23       ham\n",
       "24       ham\n",
       "25       ham\n",
       "26       ham\n",
       "27       ham\n",
       "28       ham\n",
       "29       ham\n",
       "        ... \n",
       "5542     ham\n",
       "5543     ham\n",
       "5544     ham\n",
       "5545     ham\n",
       "5546     ham\n",
       "5547    spam\n",
       "5548     ham\n",
       "5549     ham\n",
       "5550     ham\n",
       "5551     ham\n",
       "5552     ham\n",
       "5553     ham\n",
       "5554     ham\n",
       "5555     ham\n",
       "5556     ham\n",
       "5557     ham\n",
       "5558     ham\n",
       "5559     ham\n",
       "5560     ham\n",
       "5561     ham\n",
       "5562     ham\n",
       "5563     ham\n",
       "5564     ham\n",
       "5565     ham\n",
       "5566    spam\n",
       "5567    spam\n",
       "5568     ham\n",
       "5569     ham\n",
       "5570     ham\n",
       "5571     ham\n",
       "Name: label, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
